\documentclass[a4paper,12pt]{article}

% --- Подключение пакетов ---
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[russian]{babel}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{geometry}
\usepackage{xcolor}
\usepackage{listings}      % Для кода
\usepackage[most]{tcolorbox} % Для красивых рамок
\usepackage{hyperref}      % Для ссылок
\usepackage{enumitem}      % Для списков
\usepackage{booktabs}      % Для таблиц
\usepackage{float}         % Для размещения рисунков
\usepackage{tikz}          % Для рисования схем
\usetikzlibrary{positioning, shapes, arrows.meta, calc, trees, decorations.pathreplacing, matrix}

% --- Настройка полей ---
\geometry{top=1.5cm, bottom=1.5cm, left=2cm, right=2cm}

% --- ОТКЛЮЧЕНИЕ НУМЕРАЦИИ ---
\setcounter{secnumdepth}{0}

% --- Настройка ссылок ---
\hypersetup{colorlinks=true, linkcolor=blue!60!black, urlcolor=blue!60!black}

% --- Цвета и стиль кода ---
\definecolor{codegreen}{rgb}{0,0.5,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codeblue}{rgb}{0.1,0.1,0.8}
\definecolor{backcolour}{rgb}{0.97,0.97,0.97}

% --- НАСТРОЙКА КИРИЛЛИЦЫ В КОДЕ ---
\lstset{
    inputencoding=utf8,
    extendedchars=true,
    literate={а}{{\selectfont\char224}}1
             {б}{{\selectfont\char225}}1
             {в}{{\selectfont\char226}}1
             {г}{{\selectfont\char227}}1
             {д}{{\selectfont\char228}}1
             {е}{{\selectfont\char229}}1
             {ё}{{\"e}}1
             {ж}{{\selectfont\char230}}1
             {з}{{\selectfont\char231}}1
             {и}{{\selectfont\char232}}1
             {й}{{\selectfont\char233}}1
             {к}{{\selectfont\char234}}1
             {л}{{\selectfont\char235}}1
             {м}{{\selectfont\char236}}1
             {н}{{\selectfont\char237}}1
             {о}{{\selectfont\char238}}1
             {п}{{\selectfont\char239}}1
             {р}{{\selectfont\char240}}1
             {с}{{\selectfont\char241}}1
             {т}{{\selectfont\char242}}1
             {у}{{\selectfont\char243}}1
             {ф}{{\selectfont\char244}}1
             {х}{{\selectfont\char245}}1
             {ц}{{\selectfont\char246}}1
             {ч}{{\selectfont\char247}}1
             {ш}{{\selectfont\char248}}1
             {щ}{{\selectfont\char249}}1
             {ъ}{{\selectfont\char250}}1
             {ы}{{\selectfont\char251}}1
             {ь}{{\selectfont\char252}}1
             {э}{{\selectfont\char253}}1
             {ю}{{\selectfont\char254}}1
             {я}{{\selectfont\char255}}1
             {А}{{\selectfont\char192}}1
             {Б}{{\selectfont\char193}}1
             {В}{{\selectfont\char194}}1
             {Г}{{\selectfont\char195}}1
             {Д}{{\selectfont\char196}}1
             {Е}{{\selectfont\char197}}1
             {Ё}{{\"E}}1
             {Ж}{{\selectfont\char198}}1
             {З}{{\selectfont\char199}}1
             {И}{{\selectfont\char200}}1
             {Й}{{\selectfont\char201}}1
             {К}{{\selectfont\char202}}1
             {Л}{{\selectfont\char203}}1
             {М}{{\selectfont\char204}}1
             {Н}{{\selectfont\char205}}1
             {О}{{\selectfont\char206}}1
             {П}{{\selectfont\char207}}1
             {Р}{{\selectfont\char208}}1
             {С}{{\selectfont\char209}}1
             {Т}{{\selectfont\char210}}1
             {У}{{\selectfont\char211}}1
             {Ф}{{\selectfont\char212}}1
             {Х}{{\selectfont\char213}}1
             {Ц}{{\selectfont\char214}}1
             {Ч}{{\selectfont\char215}}1
             {Ш}{{\selectfont\char216}}1
             {Щ}{{\selectfont\char217}}1
             {Ъ}{{\selectfont\char218}}1
             {Ы}{{\selectfont\char219}}1
             {Ь}{{\selectfont\char220}}1
             {Э}{{\selectfont\char221}}1
             {Ю}{{\selectfont\char222}}1
             {Я}{{\selectfont\char223}}1
}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen}\itshape,
    keywordstyle=\color{codeblue}\bfseries,
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codegray},
    basicstyle=\ttfamily\small,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                     
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    aboveskip=10pt,
    belowskip=10pt,
    language=Python,
    morekeywords={def, class, import, from, return, if, elif, else, for, while, try, except, with, as, self, None, True, False, torch, nn, F, numpy, np, tensor, device, cuda},
    mathescape=true
}

% --- Блоки ---
\newtcolorbox{definitionbox}[1]{colback=green!5!white, colframe=green!40!black, fonttitle=\bfseries, title=#1, sharp corners=downhill}
\newtcolorbox{lemmabox}[1]{colback=blue!5!white, colframe=blue!40!black, fonttitle=\bfseries, title=#1}
\newtcolorbox{theorembox}[1]{colback=red!5!white, colframe=red!40!black, fonttitle=\bfseries, title=#1}

\begin{document}

% --- Титульная страница ---
\begin{titlepage}
\centering

{\Large Национальный исследовательский Нижегородский государственный университет имени Н. И. Лобачевского}

\vspace{0.5cm}

(Институт информационных технологий, математики и механики)

\vspace{1cm}

{\large Кафедра: Алгебра, геометрия и дискретная математика (АГДМ)}

\vspace{2cm}

{\Large \textbf{Обзор статьи:}}

\vspace{1cm}

{\large «Neural Sheaf Diffusion: A Topological Perspective on Heterophily and Oversmoothing in GNNs»}

\vspace{0.3cm}

(Нейронная диффузия на пучках: топологический взгляд на гетерофилию и oversmoothing в графовых нейронных сетях)

\vspace{1cm}

\textbf{Авторы:} Cristian Bodnar, Francesco Di Giovanni, Benjamin P. Chamberlain, Pietro Liò, Michael M. Bronstein

\vspace{0.3cm}

(arXiv:2202.04579v4 [cs.LG], NeurIPS 2022)

\vspace{3cm}

\begin{flushright}
Выполнил:\\
Студент группы 382351ФИ1\\
Гусев Дмитрий Алексеевич

\vspace{1cm}

Научный руководитель:\\
Мокеев Дмитрий Борисович
\end{flushright}

\vspace{2cm}

\centering
22 ноября 2025 г.

\end{titlepage}

\newpage

\tableofcontents
\newpage

\section{Введение}

\subsection{Проблема мультимодальных эмбеддингов и её актуальность}

В современной экосистеме VK существует ряд продуктов (например, VK Видео), где рекомендации формируются на основе контентных эмбеддингов разных модальностей — видео, текста, заголовков и речи. Эмбеддинг — это векторное представление объекта, которое кодирует его семантические свойства. Например, видео о котиках и текст «милые котята» должны иметь похожие эмбеддинги, чтобы система могла понять их связь. Поэтому задача построения репрезентативных мультимодальных эмбеддингов является особенно актуальной.

Традиционные подходы к объединению мультимодальных данных, такие как простое конкатенирование (склеивание векторов) или усреднение эмбеддингов, часто теряют важную структуру данных и семантические связи между различными модальностями. Например, при конкатенировании эмбеддинг видео и эмбеддинг текста просто объединяются в один длинный вектор, но при этом теряется информация о том, как эти модальности должны взаимодействовать. Необходим более совершенный механизм, который учитывает природу различных типов контента и их взаимодействие.

\subsection{Графовые нейронные сети и их ограничения}

Графовые нейронные сети (GNN) стали популярным инструментом для работы с реляционными данными и находят применение в различных областях: от социальных наук и физики частиц до структурной биологии и дизайна лекарств. GNN работают с данными, представленными в виде графа, где вершины соответствуют объектам (например, пользователям или видео), а рёбра — связям между ними (например, просмотрам или лайкам).

Однако в работе с GNN часто наблюдаются две фундаментальные проблемы:

\begin{itemize}
    \item \textbf{Плохая производительность на гетерофильных графах:} Многие GNN построены на сильном предположении гомофилии, т.е. что вершины склонны соединяться с похожими вершинами. Например, в социальной сети друзья обычно имеют схожие интересы. Однако это не всегда верно в реальных данных. В рекомендательных системах видео о котиках может быть связано с видео о собачках не потому, что они похожи, а потому, что пользователи, смотревшие одно, часто смотрят и другое. Такие графы называются гетерофильными, и классические GNN на них работают плохо.
    
    \item \textbf{Проблема oversmoothing:} При увеличении глубины GNN (количества слоёв) признаки вершин начинают становиться всё более похожими друг на друга, теряя уникальную информацию. Это происходит потому, что каждый слой усредняет признаки соседних вершин, и после многих таких усреднений все признаки становятся практически одинаковыми. В результате модель теряет способность различать разные вершины.
\end{itemize}

\subsection{Основной результат и вклад статьи}

Статья «Neural Sheaf Diffusion: A Topological Perspective on Heterophily and Oversmoothing in GNNs» (arXiv:2202.04579v4) группы исследователей (Cristian Bodnar, Francesco Di Giovanni, Benjamin P. Chamberlain, Pietro Liò, Michael M. Bronstein) представляет новый класс графовых нейронных сетей, основанный на теории пучков из алгебраической топологии.

Основной результат работы — демонстрация того, что эти две фундаментальные проблемы связаны общей причиной: тривиальной «геометрией» графа, используемой в классических GNN. Решение заключается в использовании нетривиальных пучков на графах, которые позволяют более гибко контролировать процесс передачи информации.

Статья показывает, что:
\begin{itemize}
    \item Пучковая структура позволяет избежать проблем oversmoothing и гетерофилии
    \item Дискретизированные параметрические процессы диффузии имеют больший контроль над асимптотическим поведением, чем классические GCN
    \item Пучки могут быть обучены из данных, что делает подход применимым на практике
    \item Полученные модели показывают конкурентоспособные результаты как на гетерофильных, так и на гомофильных графах
\end{itemize}

\section{Контекст и связанные работы}

\subsection{Классические графовые нейронные сети}

Классические GNN, такие как Graph Convolutional Networks (GCN) и Graph Attention Networks (GAT), можно рассматривать как использующие тривиальную структуру пучка: все вершины и рёбра имеют одинаковые векторные пространства, а отображения между ними являются тождественными (то есть просто копируют информацию без преобразования). Это означает, что информация передаётся между вершинами одинаковым образом, независимо от их свойств.

С математической точки зрения, GCN можно интерпретировать как процесс тепловой диффузии. Представьте, что признаки вершин — это температура в разных точках, и тепло распространяется по рёбрам графа, выравнивая температуру. Уравнение тепловой диффузии и его дискретизация методом Эйлера с единичным шагом:

$$\dot{X}(t) = -\Delta_0 X(t) \quad \Rightarrow \quad X(t+1) = X(t) - \Delta_0 X(t) = (I - \Delta_0)X(t)$$

где $\Delta_0$ — это нормализованный граф Лапласиан, который измеряет разницу между признаками соседних вершин.

Сравнивая это с моделью Graph Convolutional Network, мы видим, что GCN является расширенным процессом тепловой диффузии с дополнительной весовой матрицей $W$ (которая позволяет моделировать нелинейные преобразования) и нелинейностью $\sigma$ (например, ReLU):

$$\text{GCN}(X, A) := \sigma(D^{-1/2}AD^{-1/2}XW) = \sigma((I - \Delta_0)XW)$$

где $A$ — матрица смежности графа, $D$ — диагональная матрица степеней вершин.

С этой точки зрения неудивительно, что GCN особенно подвержен проблемам гетерофилии и oversmoothing: тепловая диффузия по определению стремится выровнять значения, делая признаки соседних вершин всё более похожими. Это хорошо для гомофильных графов, где похожие вершины должны иметь похожие признаки, но плохо для гетерофильных, где соседние вершины могут быть разными.


\subsection{Теория пучков и алгебраическая топология}

Пучки на графах (cellular sheaves) — это математические объекты из алгебраической топологии, которые позволяют наделить граф более богатой структурой. В отличие от классического подхода, где все вершины и рёбра обрабатываются одинаково, пучок сопоставляет каждому элементу графа своё векторное пространство и задаёт правила преобразования информации при переходе между ними.

Конкретно, пучок сопоставляет:
\begin{itemize}
    \item Векторное пространство каждой вершине графа
    \item Векторное пространство каждому ребру графа
    \item Линейное отображение между пространством вершины и пространством инцидентного ей ребра
\end{itemize}

Это позволяет, например, иметь разные размерности признаков для разных вершин или преобразовывать информацию при передаче по рёбрам, что даёт гораздо больше гибкости, чем классические GNN.


\begin{definitionbox}{Клеточный пучок}
\textbf{Определение 1.} Клеточный пучок $(G, \mathcal{F})$ на неориентированном графе $G = (V, E)$ состоит из:
\begin{itemize}
    \item Векторного пространства $\mathcal{F}(v)$ для каждой вершины $v \in V$
    \item Векторного пространства $\mathcal{F}(e)$ для каждого ребра $e \in E$
    \item Линейного отображения $\mathcal{F}_{v \trianglelefteq e}: \mathcal{F}(v) \to \mathcal{F}(e)$ для каждой инцидентной пары $v \trianglelefteq e$
\end{itemize}

Векторные пространства вершин и рёбер называются \textit{стеблями} (stalks), а линейные отображения — \textit{restriction maps}.
\end{definitionbox}

Пространство всех пространств, ассоциированных с вершинами графа, называется пространством 0-коцепей:
$$C^0(G; \mathcal{F}) := \bigoplus_{v \in V} \mathcal{F}(v)$$

Особенно важным подпространством является пространство глобальных сечений:
$$H^0(G; \mathcal{F}) := \{x \in C^0(G; \mathcal{F}) : \mathcal{F}_{v \trianglelefteq e}x_v = \mathcal{F}_{u \trianglelefteq e}x_u \text{ для всех рёбер } e = (v,u)\}$$

Это пространство содержит сигналы, которые согласованы с restriction maps пучка вдоль всех рёбер.

\subsection{Sheaf Laplacian и диффузия на пучках}

Для пучка $(G, \mathcal{F})$ можно определить оператор sheaf Laplacian, который измеряет «несогласованность мнений» в каждой вершине:

\begin{definitionbox}{Sheaf Laplacian}
\textbf{Определение 2.} Sheaf Laplacian пучка $(G, \mathcal{F})$ — это линейное отображение $L_{\mathcal{F}}: C^0(G, \mathcal{F}) \to C^0(G, \mathcal{F})$, определённое покоординатно как:
$$L_{\mathcal{F}}(x)_v := \sum_{v,u \trianglelefteq e} \mathcal{F}_{v \trianglelefteq e}^{\top}(\mathcal{F}_{v \trianglelefteq e}x_v - \mathcal{F}_{u \trianglelefteq e}x_u)$$
\end{definitionbox}

Sheaf Laplacian является положительно полуопределённой блочной матрицей. Нормализованная версия задаётся как $\Delta_{\mathcal{F}} := D^{-1/2}L_{\mathcal{F}}D^{-1/2}$, где $D$ — блочная диагональ $L_{\mathcal{F}}$.

Процесс диффузии на пучках описывается уравнением:
$$X(0) = X, \quad \dot{X}(t) = -\Delta_{\mathcal{F}}X(t)$$

В пределе времени каждая канал признаков проецируется в $\ker(\Delta_{\mathcal{F}})$ — пространство гармонических сигналов, которые согласованы с restriction maps пучка вдоль всех рёбер.

\section{Ключевые идеи нового подхода}

\subsection{Иерархия пучков и выразительная сила}

Авторы рассматривают иерархию всё более общих пучков, начиная с тривиального:

\begin{itemize}
    \item \textbf{Тривиальный пучок:} Все стебли изоморфны $\mathbb{R}$, restriction maps — тождественные. Это соответствует классическому графу Лапласиану.
    \item \textbf{Диагональные пучки:} Restriction maps являются диагональными матрицами. Упрощают вычисления, но ограничивают взаимодействие между измерениями.
    \item \textbf{Ортогональные пучки (O(d)-bundles):} Restriction maps являются ортогональными матрицами. Обеспечивают геометрическую интерпретацию и сохранение структуры данных.
    \item \textbf{Общие пучки:} Restriction maps — произвольные матрицы. Обеспечивают максимальную гибкость, но могут быть склонны к переобучению.
\end{itemize}


Авторы показывают, что по мере усложнения структуры пучка процесс диффузии на пучках может решать всё более сложные задачи классификации вершин в пределе бесконечного времени. В частности, они доказывают, что при правильной структуре пучка можно избежать проблем oversmoothing и гетерофилии.

\subsection{Гармоническое пространство и спектральный анализ}

Ключевую роль в анализе играет изучение гармонического пространства $\ker(\Delta_{\mathcal{F}})$ с спектральной точки зрения. Авторы предоставляют новое неравенство типа Cheeger для спектрального зазора sheaf Laplacian, что может представлять независимый интерес для спектральной теории пучков.

\textbf{Лемма 3.} Если $\mathcal{F}$ — дискретный O(d)-bundle над связным графом и $r := \max_{\gamma_{v \to u}, \gamma'_{v \to u}} \|P_{\gamma_{v \to u}} - P_{\gamma'_{v \to u}}\|$, то $\lambda_{\mathcal{F}}^0 \leq r^2/2$.

Следствие этого результата: всегда существует нетривиальное гармоническое пространство (т.е. $\lambda_{\mathcal{F}}^0 = 0$), если транспортные отображения, порождённые ортогональным пучком, не зависят от пути (т.е. $r = 0$).

\subsection{Обучение пучков из данных}

Одним из ключевых практических вкладов статьи является метод обучения пучков из данных. Вместо использования заранее заданных пучков, авторы предлагают обучать restriction maps с помощью параметрической функции $\Phi$:

$$\mathcal{F}_{v \trianglelefteq e} = \Phi(x_v, x_u)$$

где $x_v$ и $x_u$ — признаки вершин $v$ и $u$, инцидентных ребру $e$.

\textbf{Предложение 18.} Пусть $G = (V, E)$ — конечный граф с признаками $X$. Тогда, если $(x_v, x_u) \neq (x_w, x_z)$ для любых $(v,u) \neq (w,z) \in E$ и $\Phi$ — MLP с достаточной ёмкостью, то $\Phi$ может обучиться любому пучку $(F; G)$.

Этот результат формально мотивирует обучение пучка на каждом слое, поскольку модель может научиться различать больше вершин после каждого шага агрегации.

\section{Neural Sheaf Diffusion Model}

\subsection{Архитектура модели}

Авторы предлагают модель диффузионного типа:


$$\dot{X}(t) = -\sigma\left(\Delta_{\mathcal{F}(t)}(I_n \otimes W_1)X(t)W_2\right)$$

где $\Delta_{\mathcal{F}(t)}$ — sheaf Laplacian пучка $(G, \mathcal{F}(t))$, который эволюционирует во времени. Эволюция структуры пучка описывается обучаемой функцией данных:
$$(G, \mathcal{F}(t)) = g(G, X(t); \theta)$$

Это позволяет модели использовать последние доступные признаки для манипулирования подлежащей геометрией графа и, неявно, поведением процесса диффузии.

В экспериментах авторы фокусируются на временно дискретизированной версии этой модели:
$$X_{t+1} = X_t - \sigma\left(\Delta_{\mathcal{F}(t)}(I \otimes W_1^t)X_tW_2^t\right)$$

Эта модель отличается от классической SCN модели двумя основными способами:
\begin{enumerate}
    \item Используется обучение пучка, что делает модель применимой к любым реальным графовым датасетам
    \item Используется остаточная параметризация дискретизированного процесса диффузии, что эмпирически улучшает производительность
\end{enumerate}

\subsection{Типы SheafLearner'ов}

Для обучения restriction maps используются различные типы функций $\Phi$:

\begin{itemize}
    \item \textbf{LocalConcatSheafLearner:} Конкатенирует признаки соседних вершин и пропускает через линейный слой с активацией
    \item \textbf{AttentionSheafLearner:} Использует механизм внимания для определения весов отображений
    \item \textbf{QuadraticFormSheafLearner:} Использует квадратичную форму для обучения отображений
\end{itemize}

В зависимости от типа матриц, которые они обучают, различают:

\begin{itemize}
    \item \textbf{Диагональные отображения:} Меньше параметров на ребро, упрощённая структура sheaf Laplacian. Недостаток: измерения стеблей взаимодействуют только через умножение слева на $W_1$.
    \item \textbf{Ортогональные отображения:} Могут смешивать различные измерения стеблей, ортогональность предотвращает переобучение, лучше понимаемые теоретические свойства. В модели ортогональные матрицы строятся из композиции отражений Хаусхолдера.
    \item \textbf{Общие отображения:} Максимальная гибкость, но опасность переобучения. Sheaf Laplacian сложнее нормализовать численно, требуется использование SVD.
\end{itemize}

\subsection{Вычислительная сложность}

GCN из уравнения (2) имеет сложность $O(nc^2 + mc)$, где $c$ — число каналов, а $m$ — число рёбер. Предположим, что модель диффузии на пучках имеет размерность стебля $d$ и $f$ каналов таких, что $d \times f = c$ (т.е. тот же размер представления).

\begin{itemize}
    \item При использовании диагональных отображений сложность составляет $O(nc^2 + mdc)$
    \item При использовании ортогональных или общих матриц сложность становится $O(n(c^2 + d^3) + m(cd^2 + d^3))$
\end{itemize}

На практике авторы используют $1 \leq d \leq 5$, что эффективно приводит к постоянному накладному расходу по сравнению с GCN.

\section{Экспериментальные результаты}

\subsection{Синтетические эксперименты}

Авторы рассматривают простую установку: связный двудольный граф с равными по размеру долями. Признаки выбираются из двух перекрывающихся изотропных гауссовых распределений, чтобы сделать классы линейно неразделимыми в момент инициализации.

Из Предложения 9 известно, что модели диффузии, использующие симметричные restriction maps, не могут разделить классы в пределе, в то время как процесс диффузии, использующий отрицательные транспортные отображения, может это сделать.

Результаты показывают, что:
\begin{itemize}
    \item При времени диффузии ноль (т.е. без диффузии) линейный классификатор не может разделить классы
    \item На более поздних временах процесс диффузии, использующий симметричные отображения, не может идеально подогнать данные
    \item С более общей диффузией на пучках, по мере увеличения времени и приближения сигнала к гармоническому пространству, модель становится лучше, и признаки становятся линейно разделимыми
    \item В соответствии с Предложением 10, модель обучается отрицательному транспортному отображению для всех рёбер, что показывает, что модель способна избежать oversmoothing
\end{itemize}

\subsection{Эксперименты на реальных данных}

Авторы тестируют свои модели на множестве реальных датасетов с коэффициентом гомофилии рёбер $h$ от $h = 0.11$ (очень гетерофильный) до $h = 0.81$ (очень гомофильный). Это даёт представление о том, как модель работает на всём этом спектре.

Результаты показывают, что модели NSD (Neural Sheaf Diffusion):
\begin{itemize}
    \item Занимают первое место в 5 из 6 бенчмарков с высокой гетерофилией ($h < 0.3$)
    \item Показывают сильные результаты на гомофильных графах, находясь в пределах примерно 1\% от лучшей модели
    \item В целом, модели NSD входят в тройку лучших на 8 из 9 датасетов
    \item Модель O(d)-bundle диффузии показывает лучшие результаты в целом, подтверждая интуицию, что она может лучше избегать переобучения, одновременно трансформируя векторы достаточно сложными способами
\end{itemize}

\section{Обоснование актуальности для задачи мультимодальных пучков}

\subsection{Почему пучки подходят для мультимодальных данных}

Пучки на графах предоставляют естественный механизм для работы с мультимодальными данными:

\begin{itemize}
    \item \textbf{Гибкость представления:} Каждая вершина может иметь своё векторное пространство, соответствующее типу модальности (видео, текст, заголовок, речь)
    \item \textbf{Структурированная передача информации:} Линейные отображения на рёбрах позволяют контролируемо преобразовывать признаки между модальностями
    \item \textbf{Естественное моделирование взаимодействий:} Разные типы контента могут взаимодействовать через общие пучковые отображения
\end{itemize}

В контексте динамики мнений (opinion dynamics), $x_v$ представляет «частное мнение» вершины $v$ (например, эмбеддинг видео), а $\mathcal{F}_{v \trianglelefteq e}x_v$ выражает, как это мнение проявляется публично в «дискурсивном пространстве», образованном $\mathcal{F}(e)$ (общее пространство для взаимодействия модальностей).

\subsection{Как Neural Sheaf Diffusion помогает решить задачу}

Механизм диффузии на пучках позволяет агрегировать информацию от соседних вершин с учётом структуры пучка. Модель может научиться оптимальным способам преобразования признаков между модальностями в процессе обучения. Подход работает на графах произвольной структуры, что важно для рекомендательных систем.

Конкретные преимущества для рекомендательных систем:
\begin{itemize}
    \item \textbf{Сохранение семантики:} Пучковая структура сохраняет семантические отношения между элементами контента
    \item \textbf{Адаптивность:} Модель может адаптироваться к различным типам взаимодействий между модальностями
    \item \textbf{Интерпретируемость:} Линейные отображения пучка могут быть интерпретированы как преобразования между модальностями
    \item \textbf{Решение проблемы oversmoothing:} Пучковая структура позволяет контролировать процесс сглаживания признаков
    \item \textbf{Работа с гетерофилией:} Подход эффективен даже когда соседние вершины имеют разные типы контента
\end{itemize}

\subsection{Связь с задачей курсовой работы}

Методы из статьи Neural Sheaf Diffusion будут использованы в реализации модели для объединения мультимодальных эмбеддингов. Компоненты статьи будут адаптированы для мультимодального случая, где разные типы контента (видео, текст, заголовки, речь) взаимодействуют через общие пучковые отображения.

Ожидается, что такой подход превзойдёт простые методы агрегации за счёт:
\begin{itemize}
    \item Сохранения структуры данных и семантических связей
    \item Контролируемого преобразования признаков между модальностями
    \item Избежания проблем oversmoothing и гетерофилии
    \item Возможности интерпретации преобразований между модальностями
\end{itemize}

\section{Практические приложения и направления исследований}

\subsection{Применение к рекомендательным системам}

В контексте рекомендательных систем VK (например, VK Видео) подход на основе пучков может быть использован для:

\begin{itemize}
    \item Объединения эмбеддингов различных модальностей (видео, текст, заголовки, речь) в единое представление
    \item Улучшения качества рекомендаций за счёт учёта семантических связей между различными типами контента
    \item Построения более репрезентативных мультимодальных эмбеддингов для задач ранжирования
\end{itemize}

\subsection{Направления дальнейших исследований}

Статья открывает несколько направлений для дальнейших исследований:

\begin{itemize}
    \item \textbf{Расширение до мультимодального случая:} Адаптация подхода для работы с несколькими модальностями одновременно, где разные типы контента взаимодействуют через общие пучковые отображения
    \item \textbf{Высшие порядки sheaf Laplacians:} Использование sheaf Laplacians высших порядков, которые работают с тензорами высших порядков и могут кодировать важную информацию о подлежащих симметриях в данных
    \item \textbf{Применение к другим задачам:} Расширение подхода к другим типам данных, таким как решётки и их связанные sheaf Laplacians
    \item \textbf{Категориальная интерпретация:} Использование категориальной интерпретации пучков для расширения моделей к более экзотическим типам данных
\end{itemize}

\section{Заключение и значимость}

Работа Боднара и соавторов представляет фундаментальный вклад в область графовых нейронных сетей, объединяя методы алгебраической топологии (конкретно, теорию пучков) с современными подходами к обучению на графах.

Ключевые достижения работы:
\begin{itemize}
    \item Теоретическое обоснование связи между «геометрией» графа и проблемами гетерофилии и oversmoothing в GNN
    \item Демонстрация того, что при правильной структуре пучка можно избежать этих проблем
    \item Доказательство того, что дискретизированные параметрические процессы диффузии имеют больший контроль над асимптотическим поведением, чем классические GCN
    \item Практический метод обучения пучков из данных, делающий подход применимым на реальных датасетах
    \item Конкурентоспособные результаты на широком спектре датасетов, от сильно гетерофильных до гомофильных графов
\end{itemize}

С практической точки зрения, подход на основе пучков особенно актуален для задач объединения мультимодальных эмбеддингов в рекомендательных системах. Гибкость представления, структурированная передача информации и возможность интерпретации делают его привлекательным решением для задач, где необходимо учитывать различные типы контента и их взаимодействие.

Статья предоставляет новые связи между GNN и алгебраической топологией и будет интересна обеим областям. Для задачи курсовой работы по мультимодальным пучкам на графах эта статья служит теоретической основой и практическим руководством для реализации подхода к объединению эмбеддингов различных модальностей.

\section{Математические результаты и доказательства}

\subsection{Теоретические результаты о выразительной силе}

Авторы доказывают ряд важных результатов о способности процессов диффузии на пучках к линейному разделению классов в пределе бесконечного времени.

\textbf{Предложение 8.} Пусть $\mathcal{G}$ — множество связных графов $G = (V, E)$ с двумя классами $A, B \subset V$ таких, что для каждой $v \in A$ существует $u \in A$ и ребро $(v, u) \in E$. Тогда $H^1_{\text{sym}}$ имеет силу линейного разделения над $\mathcal{G}$.

\textbf{Предложение 9.} Пусть $\mathcal{G}$ — множество связных двудольных графов $G = (A, B, E)$ с разбиениями $A, B$, образующими два класса, и $|A| = |B|$. Тогда $H^1_{\text{sym}}$ не может линейно разделить классы любого графа в $\mathcal{G}$ для любых начальных условий $X(0) \in \mathbb{R}^{n \times f}$.

\textbf{Предложение 10.} Пусть $\mathcal{G}$ содержит все связные графы $G = (V, E)$ с двумя классами $A, B \subseteq V$. Рассмотрим пучок $(F; G) \in H^1$ с $\mathcal{F}_{v \trianglelefteq e} = -\alpha_e$ если $v \in A$ и $\mathcal{F}_{u \trianglelefteq e} = \alpha_e$ если $u \in B$ с $\alpha_e > 0$ для всех $e \in E$. Тогда диффузия, индуцированная $(F; G)$, может линейно разделить классы $G$ для почти всех начальных условий, и $H^1$ имеет силу линейного разделения над $\mathcal{G}$.

Эти результаты показывают, что при правильной структуре пучка процесс диффузии может достичь линейного разделения классов даже в сложных случаях, таких как двудольные графы, где классические GNN терпят неудачу.

\subsection{Контроль над асимптотическим поведением}

\textbf{Теорема 15.} Для $(F, G) \in H^1$, дискретизированный параметрический процесс диффузии имеет больший контроль над своим асимптотическим поведением, чем GCN. В частности, семейство SCN может, если необходимо, избежать ядра Laplacian.

\textbf{Предложение 17.} Для любого связного графа $G$ и $\varepsilon > 0$ существует пучок $(G, F) \notin H^d_{\text{sym}}$ с $\|W_1\|_2 < \varepsilon$ и вектор признаков $x$ такой, что $E_F((I \otimes W_1)x) > E_F(x)$.

Это важный результат, показывающий, что не только диффузия на пучках более выразительна, чем тепловая диффузия, но и SCN более выразительны, чем GCN, в том смысле, что они, как правило, не ограничены уменьшением энергии Дирихле при использовании весов с малой нормой.

\subsection{Спектральный анализ и неравенство Cheeger}

Авторы предоставляют новое неравенство типа Cheeger для спектрального зазора sheaf Laplacian:

\textbf{Предложение 3.} Если $F$ — дискретный O(d)-bundle над связным графом и $r := \max_{\gamma_{v \to u}, \gamma'_{v \to u}} \|P_{\gamma_{v \to u}} - P_{\gamma'_{v \to u}}\|$, то $\lambda_{\mathcal{F}}^0 \leq r^2/2$.

Этот результат связывает спектральный зазор sheaf Laplacian с зависимостью транспортных отображений от пути, что является ключевым для понимания свойств гармонического пространства.

\section{Детальное описание экспериментальных результатов}

\subsection{Синтетические эксперименты}

В синтетических экспериментах авторы рассматривают связный двудольный граф с равными по размеру долями. Признаки выбираются из двух перекрывающихся изотропных гауссовых распределений, чтобы сделать классы линейно неразделимыми в момент инициализации.

Результаты показывают:
\begin{itemize}
    \item При времени диффузии ноль (без диффузии) линейный классификатор не может разделить классы
    \item На более поздних временах процесс диффузии, использующий симметричные отображения, не может идеально подогнать данные
    \item С более общей диффузией на пучках, по мере увеличения времени и приближения сигнала к гармоническому пространству, модель становится лучше, и признаки становятся линейно разделимыми
    \item Гистограмма всех транспортных (скалярных) отображений показывает, что модель обучается отрицательному транспортному отображению для всех рёбер, в соответствии с Предложением 10
\end{itemize}

\subsection{Результаты на реальных датасетах}

Авторы тестируют модели на 9 датасетах с коэффициентом гомофилии рёбер от $h = 0.11$ (очень гетерофильный) до $h = 0.81$ (очень гомофильный):

\begin{itemize}
    \item \textbf{Гетерофильные датасеты ($h < 0.3$):} Texas ($h = 0.11$), Wisconsin ($h = 0.21$), Film ($h = 0.22$), Squirrel ($h = 0.22$), Chameleon ($h = 0.22$), Cornell ($h = 0.23$)
    \item \textbf{Гомофильные датасеты ($h > 0.7$):} Citeseer ($h = 0.74$), Pubmed ($h = 0.80$), Cora ($h = 0.81$)
\end{itemize}

\textbf{Ключевые результаты:}
\begin{itemize}
    \item Модели NSD занимают первое место в 5 из 6 бенчмарков с высокой гетерофилией
    \item На оставшемся датасете (Chameleon) модели NSD занимают второе место
    \item На гомофильных графах NSD показывает сильные результаты, находясь в пределах примерно 1\% от лучшей модели
    \item В целом, модели NSD входят в тройку лучших на 8 из 9 датасетов
    \item Модель O(d)-bundle диффузии показывает лучшие результаты в целом, подтверждая интуицию, что она может лучше избегать переобучения, одновременно трансформируя векторы достаточно сложными способами
    \item Модель, обучающая диагональные отображения, также показывает сильные результаты, несмотря на более простую функциональную форму Laplacian
\end{itemize}

\subsection{Сравнение с baseline-методами}

Модели NSD сравниваются с широким набором GNN моделей, которые можно разделить на три категории:

\begin{enumerate}
    \item \textbf{Классические модели:} GCN, GAT, GraphSAGE
    \item \textbf{Модели, специально разработанные для гетерофильных настроек:} GGCN, Geom-GCN, H2GCN, GPRGNN, FAGCN, MixHop
    \item \textbf{Модели, решающие проблему oversmoothing:} GCNII, PairNorm
\end{enumerate}

Результаты показывают, что подход на основе пучков превосходит или конкурирует с этими методами на широком спектре датасетов, демонстрируя универсальность подхода.

\section{Связанные работы и обсуждение}

\subsection{Sheaf Neural Networks и обучение пучков}

Sheaf Neural Networks с заранее заданным sheaf Laplacian были первоначально введены в игрушечной экспериментальной установке. В отличие от предыдущих работ, авторы предоставляют обширный теоретический анализ, обосновывающий использование пучков в Graph ML, и впервые изучают, как пучки могут быть обучены из данных с использованием нейронных сетей.

Последующие работы также экспериментировали с выводом connection Laplacian непосредственно из данных на этапе предобработки, комбинированием пучков с вниманием и проектированием моделей на основе волнового уравнения на пучках.

\subsection{Гетерофилия и Oversmoothing}

Хотя хорошие эмпирические конструкции, совместно решающие эти две проблемы, были предложены ранее, работа Yan et al. [72] — единственная другая работа, теоретически связывающая эти два явления. Однако их анализ очень отличается по методам и предположениям, и поэтому их результаты полностью ортогональны.

Авторы используют новый набор математических инструментов из теории клеточных пучков, что привносит новый язык и новые инструменты для анализа этих проблем.

\subsection{Категориальная теория и GNN}

С точки зрения категориальной теории, клеточные пучки являются функтором из категории, описывающей структуру инцидентности графа, в категорию, описывающую данные, живущие на графе. Эта общность может быть использована для расширения моделей к более экзотическим типам данных, таким как решётки и их связанные sheaf Laplacians.

\section{План реализации для курсовой работы}

\subsection{Этап 1: Реализация базового алгоритма для одной модальности}

На первом этапе необходимо реализовать базовый алгоритм обучения пучка на графе для работы с одной модальностью. Это позволит получить практический опыт работы с концепцией пучков и проверить корректность реализации.

\textbf{Основные компоненты:}
\begin{enumerate}
    \item Реализация структуры данных для представления пучка (стебли вершин и рёбер, restriction maps)
    \item Реализация SheafLearner для обучения restriction maps (начнём с LocalConcatSheafLearner)
    \item Реализация процесса диффузии на пучке
    \item Реализация sheaf Laplacian и его нормализованной версии
    \item Базовый цикл обучения с обратным распространением ошибки
\end{enumerate}

\textbf{Используемые инструменты:}
\begin{itemize}
    \item PyTorch для реализации нейронных сетей
    \item PyTorch Geometric для работы с графами
    \item NumPy для численных вычислений
\end{itemize}

\subsection{Этап 2: Расширение до мультимодального случая}

На втором этапе модель расширяется для работы с несколькими модальностями одновременно.

\textbf{Ключевые модификации:}
\begin{enumerate}
    \item Определение различных типов стеблей для разных модальностей (видео, текст, заголовки, речь)
    \item Реализация общих пучковых отображений для преобразования между модальностями
    \item Адаптация процесса диффузии для работы с мультимодальными признаками
    \item Механизм объединения мультимодальных эмбеддингов после диффузии
\end{enumerate}

\subsection{Этап 3: Эксперименты и оценка}

На третьем этапе проводятся эксперименты на открытых рекомендательных датасетах.

\textbf{Экспериментальная установка:}
\begin{itemize}
    \item Выбор датасетов с мультимодальными данными (например, MovieLens с метаданными фильмов)
    \item Предобработка данных: извлечение эмбеддингов для различных модальностей
    \item Построение графа взаимодействий (пользователь-контент, контент-контент)
    \item Разделение данных на обучающую, валидационную и тестовую выборки
\end{itemize}

\textbf{Метрики качества:}
\begin{itemize}
    \item Recall@K — доля релевантных элементов среди первых K рекомендаций
    \item NDCG (Normalized Discounted Cumulative Gain) — нормализованный дисконтированный накопительный выигрыш
    \item MRR (Mean Reciprocal Rank) — средний обратный ранг
\end{itemize}

\textbf{Baseline-методы для сравнения:}
\begin{itemize}
    \item Простые методы агрегации (конкатенация, усреднение эмбеддингов)
    \item Классические GNN (GCN, GAT) без учёта мультимодальности
    \item Другие методы объединения мультимодальных эмбеддингов (early fusion, late fusion, cross-modal attention)
\end{itemize}

\section{Заключение и значимость}

Работа Боднара и соавторов представляет фундаментальный вклад в область графовых нейронных сетей, объединяя методы алгебраической топологии с современными подходами к обучению на графах. Теоретические результаты о связи между «геометрией» графа и проблемами гетерофилии и oversmoothing, а также практический метод обучения пучков из данных делают эту работу особенно ценной.

Для задачи курсовой работы по мультимодальным пучкам на графах эта статья служит:
\begin{itemize}
    \item \textbf{Теоретической основой} для понимания того, как пучки могут быть использованы для работы с различными типами данных
    \item \textbf{Практическим руководством} для реализации подхода к объединению эмбеддингов различных модальностей
    \item \textbf{Источником вдохновения} для адаптации методов к специфическим задачам рекомендательных систем
\end{itemize}

Ожидается, что применение подходов из статьи к задаче объединения мультимодальных эмбеддингов позволит создать более эффективные модели рекомендательных систем, способные учитывать семантические связи между различными типами контента и избегать проблем, характерных для классических GNN.

\end{document}


